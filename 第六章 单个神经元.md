# 第六章 单个神经元
## 6.1 神经元的拟合原理
z=wx+b

正向传播：随机赋值w,b，得到预测输出z

反向传播：预测输出z与真实值y作比较，将误差反向传播，不断修正w,b。至于每次调整多少，引入“学习率”参数来控制
## 6.2 激活函数——加入非线性因素，解决线性模型缺陷
神经网络常用的激活函数有：Sigmoid、Tanh、Relu等

(1)Sigmoid函数 能够将输出压缩到0~1之间，TensorFlow中对应函数为：tf.nn.sigmoid(x,name=None)

(2)Tanh函数 Sigmoid函数的值域升级版，能够将输出压缩到-1~1之间，TensorFlow中对应函数为：tf.nn.tanh(x,name=None)

对于以上两个函数，以Sigmoid为例：理论上讲x取值范围是负无穷到正无穷，而且x趋于正负无穷时，对应的y值越接近1或0，这种情况称为饱和。也就是说当x=100和当x=1000时输出结果是一样的，这相当于丢失了1000大于100十倍这个信息。因此，使用这两个函数时一定要注意输入的x值不能过大，否则模型无法训练。

(3)ReLU函数 f(x)=max(0,x)  tf.nn.relu(features,name=None)
            f(x)=min(max(features,0),6)  tf.nn.relu6(features,name=None)
            
除此之外还有Softplus、Elus、Leaky relus

(3）Swish函数 谷歌公司发现的效果优于ReLU的激活函数，在TensorFlow中可以手动封装(书103页）


总的来说，目前ReLU函数最常用。

## 6.3 softmax算法——处理分类问题
二分类问题——Sigmoid

多分类问题（互斥）——Softmax

非互斥的多分类问题——用多个二分类来组成

Softmax：若判断输入属于某一类的概率大于属于其它类的概率，那么这个类对应的值就逼近1，其余类的值就逼近0。因此，在实际使用中，Softmax伴随的标签分类都为one_hot编码。

## 6.4 损失函数——用真实值与预测值的距离指导模型的收敛方向
损失函数用于描述真实值与预测值之间差距的大小，常见的两种算法是：均值方差（MSE）和交叉熵

算法的选取取决于输入标签数据的类型:对于实数、无界限的值，使用均方误差比较合适；如果输入标签是位矢量（分类标志），那么使用交叉熵会更合适

## 6.5 Softmax算法与损失函数的综合应用
### 6.5.1 交叉熵实验
labes代表数据的真实标签，logits代表神经网络的输出值


```python
import tensorflow as tf

labels=[[0,0,1],[0,1,0]]
logits=[[2,0.5,6],[0.1,0,3]]      # 第一个输出对应的标签是正确的，第二个是错误的

logits_scaled=tf.nn.softmax(logits)

result1=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)
result2=-tf.reduce_sum(labels*tf.log(logits_scaled),1)

with tf.Session() as sess:
    print("logits_scaled:",sess.run(logits_scaled))
    print("result1:",sess.run(result1))
    print("result2:",sess.run(result2))
```

    WARNING:tensorflow:From <ipython-input-1-43889a731157>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    
    Future major versions of TensorFlow will allow gradients to flow
    into the labels input on backprop by default.
    
    See `tf.nn.softmax_cross_entropy_with_logits_v2`.
    
    logits_scaled: [[0.01791432 0.00399722 0.97808844]
     [0.04980332 0.04506391 0.90513283]]
    result1: [0.02215516 3.0996735 ]
    result2: [0.02215518 3.0996735 ]
    

根据logits_scaled：原本logits里的值加和大于1，经过Softmax之后，加和等于1

根据result1：由于第一个样本与标签分类相符，所以交叉熵比较小，为0.02215516；第二个反之。而且，传入softmax_cross_entropy_with_logits的logits不需要进行Softmax，因为该函数已经包含Softmax操作了。也就是说，若要使用Softmax转化过的logits_scaled计算交叉熵，则需要自己写loss参数实现，参见result2，所得结果与softmax_cross_entropy_with_logits实现的一致

### 6.5.2 one_hot实验
当输入标签不是标准的独热码时，我们对它进行交叉熵，比较非独热码标签与独热码标签的区别:


```python
labels3=[[0.4,0.1,0.5],[0.3,0.6,0.1]] #n not one hot 
# logits=[[2,0.5,6],[0.1,0,3]]  #第一个与标签分类相符，第二个不相符

result3=tf.nn.softmax_cross_entropy_with_logits(labels=labels3,logits=logits)

with tf.Session() as sess:
    print("result3:",sess.run(result3))
```

    result3: [1.2350768 2.7696736]
    

当标签为独热码时，交叉熵结果[result1: [0.02215516 3.0996735 ]，当标签为非独热码时，result3: [1.2350768 2.7696736]。对于分类正确的交叉熵和分类错误的交叉熵，二者对比没有那么明显了。因此，还是使用独热码标签比较好。
### 6.5.3 计算loss值


```python
loss=tf.reduce_mean(result1)   #用result计算
with tf.Session() as sess:
    print("loss=",sess.run(loss))
```

    loss= 1.5609143
    


```python
loss2=tf.reduce_mean(-tf.reduce_sum(labels*tf.log(logits_scaled),1)) #用logits_scaled计算
with tf.Session()as sess:
    print("loss2:",sess.run(loss2))
```

    loss2: 1.5609144
    

## 6.6 梯度下降
在TensorFlow中是通过一个叫Optimizer的优化器进行训练优化的，例如：

optimizer=tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

在这里，learning_rate是学习率。学习率设置的过大，则训练速度会提升，但精度不够；学习率设置过小，精度会提升但训练时间会过长。为了在速度和精度之间找到一个平衡，我们可以使用【退化学习率】，也就是训练刚开始的时候使用大的学习率加快速度，训练到一定程度后，使用小的学习率增加精度：

tf.train.exponential_decay(initoal_learning_rate,global_step,decay_step,decay_rate)

例如：我们设置初始学习率为0.1，令其以每5次衰减0.9的速度来退化，代码如下


```python
import tensorflow as tf
initial_learning_rate=0.1
global_step=tf.Variable(0,trainable=False)

learning_rate=tf.train.exponential_decay(initial_learning_rate,global_step=global_step,decay_steps=5,decay_rate=0.9)
optimizer=tf.train.GradientDescentOptimizer(learning_rate)
add_global=global_step.assign_add(1)  # +1

with tf.Session() as sess:
    tf.global_variables_initializer().run()  #初始化所有参数
    for i in range(20):
        g,rate=sess.run([add_global,learning_rate])
        print(g,rate)   
```

    1 0.1
    2 0.095873155
    3 0.09387404
    4 0.09387404
    5 0.089999996
    6 0.08812335
    7 0.08628584
    8 0.08628584
    9 0.08272495
    10 0.08099999
    11 0.07931101
    12 0.077657245
    13 0.076037966
    14 0.076037966
    15 0.07445245
    16 0.071379915
    17 0.071379915
    18 0.06989152
    19 0.06700721
    20 0.06560999
    
